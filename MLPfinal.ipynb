{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MLPfinal.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"199vpwfvka_7","colab_type":"code","colab":{}},"cell_type":"code","source":["!pip install -q xlrd  # biblioteca para extraer y leer los datos de excel\n","!pip install pycm # biblioteca para el analisis estadistico de matrices de confusión\n","!git clone https://github.com/github466/DL.git #clonar el repositorio con los archivos de la BD "],"execution_count":0,"outputs":[]},{"metadata":{"id":"YWNDvW9RkV_j","colab_type":"code","colab":{}},"cell_type":"code","source":["import csv #libreria para lectura de archivos en formato csv \n","import itertools #libreria que convierte los datos a iterables de forma mas rapida \n","from tensorflow.examples.tutorials.mnist import input_data #importar función imput data de los ejemplos de mnist de tensorflow\n","import numpy as np #biblioteca para trabajar con vectores y matrices \n","import tensorflow as tf #entorno de trabajo de codigo abierto para el dieño, construcción y desarrollo de modelos de DeepLearning\n","import matplotlib.pyplot as plt#biblioteca para la generación de graficos a partir de datos contenidos en listas o arrays\n","import pylab #Api similar a la de generación de graficos de Matlab\n","import os #El módulo os nos permite acceder a funcionalidades dependientes del Sistema Operativo\n","from sklearn.metrics import confusion_matrix # libreria para genera la matriz de confusión\n","from pycm import *# biblioteca para el analisis estadistico de matrices de confusión"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6kqDO1TIhSaC","colab_type":"code","colab":{}},"cell_type":"code","source":["def plot_confusion_matrix(cm, classes,   #función para generar la matriz de confusión para evaluar el clasificador\n","                          normalize=False,\n","                          title='Confusion matrix',\n","                          cmap=plt.cm.Blues):\n","    \"\"\"\n","    This function prints and plots the confusion matrix.\n","    Normalization can be applied by setting `normalize=True`.\n","    \"\"\"\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","        print(\"Normalized confusion matrix\")\n","    else:\n","        print('Confusion matrix, without normalization')\n","\n","    print(cm)\n","\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","    tick_marks = np.arange(len(classes))\n","    plt.xticks(tick_marks, classes, rotation=45)\n","    plt.yticks(tick_marks, classes)\n","\n","    fmt = '.2f' if normalize else 'd'\n","    thresh = cm.max() / 2.\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        plt.text(j, i, format(cm[i, j], fmt),\n","                 horizontalalignment=\"center\",\n","                 color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')\n","    plt.tight_layout()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sO3jAChZhS8H","colab_type":"code","colab":{}},"cell_type":"code","source":["def acurrency(_logits,_y,_data_x,_data_y):\n","  pred = tf.nn.softmax(_logits)  # Apply softmax to logits\n","  correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(_y, 1))\n","  # Calculate accuracy\n","  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n","  eval = accuracy.eval({X: _data_x, Y: _data_y})  \n","  return eval"],"execution_count":0,"outputs":[]},{"metadata":{"id":"biIcy25nhS-Y","colab_type":"code","colab":{}},"cell_type":"code","source":["def generateLayers(n_input,n_classes,n_hidden): #función para generar las capas del perceptron\n","    weights = {}   \n","    biases = {}\n","    hidden_before=n_input\n","    for i in range(len(n_hidden)):\n","        l=i+1        \n","        weights['h' + str(l)]=tf.Variable(tf.random_normal([hidden_before, n_hidden[i]]))      \n","        biases['b' + str(l)]=tf.Variable(tf.random_normal([n_hidden[i]]))\n","        hidden_before = n_hidden[i]\n","        \n","    weights['out']=tf.Variable(tf.random_normal([hidden_before,n_classes]))\n","    biases['out']=tf.Variable(tf.random_normal([n_classes]))\n","    return weights,biases"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2xZKO0OdhTAy","colab_type":"code","colab":{}},"cell_type":"code","source":["def metrics(_sess,_logict,x_data,y_data): #función para generar los valores de especificidad y sensibilidad\n","    pred=tf.argmax(_logict,1)\n","    y_pred = pred.eval(feed_dict={X: x_data})\n","    t_y_real = tf.argmax(y_data,1)\n","    y_real = sess.run(t_y_real)\n","    print(type(y_pred),type(y_real))\n","    cm = ConfusionMatrix(y_real.tolist(), y_pred.tolist())\n","    #cm.TNR Especificidad o verdadesros negativos\n","    #cm.TPR Sensibilidad o verdaderos positivos\n","    return cm,cm.TNR,cm.TPR,y_real.tolist(), y_pred.tolist()\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hIDvzA1FhTDQ","colab_type":"code","colab":{}},"cell_type":"code","source":["def multilayer_perceptron(x_,n_hidden_,weights_,biases_,dropout_rate_):  #función para generar el perceptron multicapa\n","    layer_before = x_\n","    for i in range(len(n_hidden_)):\n","        l=i+1   \n","        layer = tf.sigmoid( tf.add(tf.matmul(layer_before, weights_['h'+ str(l)]), biases_['b'+ str(l)]) )     #función sigmoidal en todas la capas del MLP\n","        layer_before=layer   \n","    # Output fully connected layer with a neuron for each class\n","    #drop_out = tf.nn.dropout(layer_before, dropout_rate)  # DROP-OUT here\n","    #out_layer = tf.matmul(drop_out, weights['out']) + biases['out']\n","    out_layer = tf.matmul(layer_before, weights_['out']) + biases_['out']\n","    out_layer = tf.sigmoid(out_layer) #funcion de activación sigmoidal en la capa de salida \n","#    sigmoide o softmax\n","    out_layer =  tf.nn.dropout(out_layer, dropout_rate_)  # DROP-OUT en la capa de salida \n","    #dropout .5 .3 .2\n","    return out_layer"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sK1w1TIPhS2w","colab_type":"code","colab":{}},"cell_type":"code","source":["def load_data_train_test(porcentTestLength,name):#función para leer los datos de train y de test ya que estan en dos archivos separados \n","    datos=[]\n","    with open(name) as f:\n","        reader = csv.reader(f)\n","        for row in reader:\n","            if(row!=','):               \n","                datos.append(row)\n","    \n","    \n","    size = len(datos)    \n","    testLength = round(size*porcentTestLength/100)\n","    np.random.shuffle(datos)\n","\n","    test_x,test_y = separeData(datos[:testLength])\n","    train_x,train_y = separeData(datos[testLength:]) \n","    return datos,train_x,train_y, test_x,test_y\n","def load_data(name): \n","    datos=[]\n","    with open(name) as f:\n","        reader = csv.reader(f)\n","        for row in reader:\n","            if(row!=','):               \n","                datos.append(row)\n","    \n","    \n","   \n","    np.random.shuffle(datos)\n","    test_x,test_y = separeData(datos)   \n","    return datos,test_x,test_y\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6_oLXW9-hS5f","colab_type":"code","colab":{}},"cell_type":"code","source":["def separeData(datos):   #Función para separar las caracteristicas de entrada de las clases, \"separación vertical\"\n","    \n","    X=[]\n","    S=[]\n","    for d in datos:\n","        X.append(d[0:178])\n","        S.append(d[178:len(datos[0])])   \n","    \n","   \n","    X=np.array(X, np.float64)\n","    S=np.array(S,np.float64)\n","    \n","    return X,S"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fZ6pZA5n0G_w","colab_type":"code","colab":{}},"cell_type":"code","source":["_,train_x_orig, train_y_orig = load_data('DL/train.csv')\n","_,test_x_orig, test_y_orig = load_data('DL/test.csv')\n","print(test_x_orig.shape) #imprimir los datos de test \n","print(test_x_orig[0])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sU-scYfShTGA","colab_type":"code","colab":{}},"cell_type":"code","source":["_,train_x_orig, train_y_orig = load_data('DL/train.csv')\n","_,test_x_orig, test_y_orig = load_data('DL/test.csv')\n","\n","\n","print(train_x_orig.shape)\n","total_examples = len(train_x_orig) # numero de ejemplos de la BD\n","learning_rate = 0.0001 #\n","training_epochs = 3000 #\n","batch_size =30# <=10 & <= 50 # numero de ejemplos que van a ser mostrados a la red en cada itereación \n","display_step = 10 #cada cuantas iteraciónes imprime los resultados de la función de costo \n","n_input = 178  #numero de caracteristicas de la capa de entrada \n","n_hidden=[600,500,400,300] # numero de capas oculta y cantidad de neuronas por capa \n"," n_classes = 5 # numero de clases \n","dropout_rate = 0.5 #.5 .3 .2 #cantidad de dropout, tecnica para la regularización por medio del apagado de algunas neuronas que pueden estar saturadas \n","\n","# tf Graph input\n","X = tf.placeholder(\"float\", [None, n_input],\"X\") #definición de los placeholders o espacios en el grafo para los datos de entrada \n","Y = tf.placeholder(\"float\", [None, n_classes],\"Y\")#definicion de los place holders o espacios en el grafo para los datos \n","\n","# Store layers weight & bias\n","weights,biases = generateLayers(n_input,n_classes,n_hidden)\n","# Construct model\n","logits = multilayer_perceptron(X,n_hidden,weights,biases,dropout_rate)\n","\n","# Define loss and optimizer\n","\n","loss_op = tf.reduce_mean(tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=Y))#\n","optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)# optimizador \n","train_op = optimizer.minimize(loss_op)\n","\n","\n","# Initializing the variables\n","init = tf.global_variables_initializer()\n","\n","with tf.Session() as sess:\n","    #saver = tf.train.Saver()\n","    sess.run(init)\n","    epoch_min=cost_init=accurrency_init=accurrency_current=0\n","    cost_min=99999999 \n","    costs=accurrencys=[]  \n","    cost_bef=-cost_min    \n"," \n","    # Training cycle\n","    for epoch in range(training_epochs):\n","        avg_cost = 0.\n","        total_batch = int(len(train_x_orig)/batch_size)            \n","        # Loop over all batches\n","        for i in range(total_batch):\n","            start=i*batch_size\n","            end=(i+1)*batch_size \n","            batch_x=train_x_orig[start:end]\n","            batch_y=train_y_orig[start:end]\n","            # Run optimization op (backprop) and cost op (to get loss value)\n","            _, c = sess.run([train_op, loss_op], feed_dict={X: batch_x,Y: batch_y}) \n","            # Compute average loss\n","            avg_cost += c / total_batch\n","        # Display logs per epoch step\n","        \n","        if(avg_cost < cost_min):\n","          cost_min=avg_cost\n","          epoch_min=epoch          \n","        if epoch % display_step == 0:\n","            simbol = \"+\" if cost_bef<avg_cost else \"-\"            \n","            print(simbol,\") Epoch:\", '%04d' % (epoch+1), \"cost={:.9f}\".format(avg_cost),\"accurrency={:.9f}\".format(accurrency_current))           \n","            cost_bef=avg_cost \n","        if(epoch==0):\n","          cost_init = avg_cost\n","          accurrency_init =  acurrency(logits,Y,test_x_orig,test_y_orig)\n","          \n","        costs.append(avg_cost)\n","        accurrencys.append(accurrency_current)\n","    print(\"Optimization Finished!\")    \n","    print(\"Cost Min:\",\"cost={:.9f}\".format(cost_min),\" in epoch:\",'%04d' % (epoch_min+1),\"cost_init={:.9f}\".format(cost_init),\"accurrency_init={:.9f}\".format(accurrency_init));\n","    # Test model\n","    pred = tf.nn.softmax(logits)  # Apply softmax to logits\n","    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1))\n","    # Calculate accuracy\n","    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n","    print(\"Accuracy-train:\", acurrency(logits,Y,train_x_orig,train_y_orig)) \n","    print(\"Accuracy-test:\", acurrency(logits,Y,test_x_orig,test_y_orig)) \n","   \n","    cm,TNR,TPR,y_real,y_pred = metrics(sess,logits,test_x_orig,test_y_orig)\n","    cnf_matrix = confusion_matrix(y_real, y_pred)   \n","    print(\"TNR Especificidad o verdadesros negativos\",TNR)\n","    print(\"TPR Sensibilidad o verdaderos positivos\",TPR)    \n","    print(cm)\n","    plot_confusion_matrix(cnf_matrix, classes=cm.classes,title='Confusion matrix')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_EJu-mx80toq","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"pkDSV23Nz_7u","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"gDvaxTcZhTIh","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}